#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WPSæ–‡ä»¶æ–‡æœ¬æå–å™¨ - ç²¾ç®€ç‰ˆ
ä¸»è¦å…¥å£ï¼šæ–‡ä»¶æµ
åŠŸèƒ½ï¼šä»WPSæ–‡ä»¶æµä¸­æå–çº¯å‡€çš„æ–‡æœ¬å†…å®¹
"""

import os
import re
import tempfile

def extract_text_from_wps_stream(byte_stream: bytes) -> str:
    """
    ä»WPSæ–‡ä»¶å­—èŠ‚æµä¸­æå–æ–‡æœ¬å†…å®¹
    
    Args:
        byte_stream: WPSæ–‡ä»¶çš„å­—èŠ‚æµæ•°æ®
    
    Returns:
        æå–çš„çº¯å‡€æ–‡æœ¬å†…å®¹
    """
    try:
        # æ–¹æ³•1: ä½¿ç”¨OLEå¤åˆæ–‡æ¡£è§£æï¼ˆä¼˜å…ˆæ–¹æ³•ï¼‰
        ole_text = _extract_with_ole(byte_stream)
        if ole_text and len(ole_text.strip()) > 20:
            return _clean_text(ole_text)
        
        # æ–¹æ³•2: ç›´æ¥äºŒè¿›åˆ¶è§£æï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        binary_text = _extract_with_binary(byte_stream)
        if binary_text and len(binary_text.strip()) > 10:
            return _clean_text(binary_text)
        
        return "æœªèƒ½ä»WPSæ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å†…å®¹"
        
    except Exception as e:
        return f"æå–å¤±è´¥: {str(e)}"

def _extract_with_ole(file_data: bytes) -> str:
    """ä½¿ç”¨OLEå¤åˆæ–‡æ¡£è§£æ"""
    try:
        import olefile
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wps') as tmp_file:
            tmp_file.write(file_data)
            tmp_file_path = tmp_file.name
        
        try:
            if not olefile.isOleFile(tmp_file_path):
                return ""
            
            ole = olefile.OleFileIO(tmp_file_path)
            text_results = []
            
            # è§£æWordDocumentæµ
            if ole.exists('WordDocument'):
                word_doc_stream = ole.openstream('WordDocument')
                stream_data = word_doc_stream.read()
                text = _parse_stream_data(stream_data)
                if text:
                    text_results.append(text)
                word_doc_stream.close()
            
            ole.close()
            return '\n'.join(text_results)
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_file_path):
                os.unlink(tmp_file_path)
        
    except ImportError:
        return ""
    except Exception:
        return ""

def _extract_with_binary(file_data: bytes) -> str:
    """ç›´æ¥äºŒè¿›åˆ¶è§£æ"""
    try:
        return _parse_stream_data(file_data)
    except Exception:
        return ""

def _parse_stream_data(data: bytes) -> str:
    """è§£ææµæ•°æ®æå–æ–‡æœ¬"""
    text_segments = []
    
    # UTF-16LEè§£ç ï¼ˆWPSä¸»è¦ç¼–ç ï¼‰
    try:
        text = data.decode('utf-16le', errors='ignore')
        clean_text = _extract_meaningful_text(text)
        if clean_text:
            text_segments.append(clean_text)
    except:
        pass
    
    # UTF-8è§£ç 
    try:
        text = data.decode('utf-8', errors='ignore')
        clean_text = _extract_meaningful_text(text)
        if clean_text:
            text_segments.append(clean_text)
    except:
        pass
    
    # GBKè§£ç 
    try:
        text = data.decode('gbk', errors='ignore')
        clean_text = _extract_meaningful_text(text)
        if clean_text:
            text_segments.append(clean_text)
    except:
        pass
    
    return '\n'.join(text_segments)

def _extract_meaningful_text(text: str) -> str:
    """æå–æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹"""
    if not text or len(text.strip()) < 5:
        return ""
    
    # åˆ†å‰²æ–‡æœ¬ä¸ºæ®µè½
    paragraphs = re.split(r'[\x00-\x1f]+', text)
    meaningful_segments = []
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if len(paragraph) < 5:
            continue
        
        # è¿‡æ»¤æ ¼å¼åŒ–å­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹
        clean_paragraph = ''.join(c for c in paragraph 
                                if c.isprintable() or c in '\n\t ')
        
        if not clean_paragraph.strip():
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„å†…å®¹
        has_chinese = bool(re.search(r'[\u4e00-\u9fff]', clean_paragraph))
        has_english_words = bool(re.search(r'[a-zA-Z]{2,}', clean_paragraph))
        
        # è¿‡æ»¤å…ƒæ•°æ®å’Œæ ¼å¼ä¿¡æ¯
        if _is_metadata(clean_paragraph):
            continue
        
        if has_chinese or has_english_words:
            meaningful_segments.append(clean_paragraph)
    
    return '\n'.join(meaningful_segments)

def _is_metadata(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå…ƒæ•°æ®æˆ–æ ¼å¼ä¿¡æ¯"""
    metadata_patterns = [
        r'CJOJPJQJ',
        r'Root Entry',
        r'SummaryInformation',
        r'DocumentSummaryInformation',
        r'KSOProductBuildVer',
        r'Times New Roman',
        r'Calibri',
        r'Arial',
        r'Courier New',
        r'HTML.*ä»£ç ',
        r'mH.*sH.*nHtH',
        r'^\s*[0-9@#$%^&*()_+=\[\]{}|\\:";\'<>?,./`~-]+\s*$',
        r'^\s*[A-Za-z0-9@#$%^&*()_+=\[\]{}|\\:";\'<>?,./`~-]{1,3}\s*$'
    ]
    
    return any(re.search(pattern, text, re.IGNORECASE) for pattern in metadata_patterns)

def _is_garbled_text(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºä¹±ç """
    if not text or len(text) < 3:
        return True
    
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.sub(r'\s', '', text))
    
    if total_chars == 0:
        return True
        
    chinese_ratio = chinese_chars / total_chars
    
    # å¦‚æœä¸­æ–‡å­—ç¬¦æ¯”ä¾‹å¤ªä½ï¼Œå¯èƒ½æ˜¯ä¹±ç 
    if chinese_ratio < 0.4:
        return True
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„ç‰¹æ®Šç¬¦å·
    special_chars = len(re.findall(r'[`\\/_<>{}|~]', text))
    if special_chars > 3:
        return True
    
    # æ£€æŸ¥æ˜æ˜¾çš„ä¹±ç æ¨¡å¼
    garbled_patterns = [
        r'[A-Za-z]{2,}\d+[A-Za-z]{2,}',  # å­—æ¯æ•°å­—æ··åˆ
        r'[`\\/_<>{}|~]{2,}',  # è¿ç»­ç‰¹æ®Šå­—ç¬¦
        r'[\u4e00-\u9fff][A-Za-z]{1}[\u4e00-\u9fff][A-Za-z]{1}',  # ä¸­è‹±æ··æ‚æ¨¡å¼
        r'å‰‰|è«²|é´™|è›»|é¢¯|éŒ˜|å»±|éŠ|æ|è³¼|æ±µ|èŸ¢|è›“|ç­«|èª°|é¾•|é™™|é¦·|ä¿Œ|æ¸ˆ|çƒ»|è–¡|éµ|åƒ|äº¯|æ“½|æ¯|ç­Ÿ|é¯ª|é­|é¬´|æ˜¢|è§º|åˆ§'  # æ˜æ˜¾çš„ä¹±ç å­—ç¬¦
    ]
    
    for pattern in garbled_patterns:
        if re.search(pattern, text):
            return True
    
    return False

def _clean_text(text):
    """æ™ºèƒ½æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™ä¸­æ–‡æ–‡æœ¬å—å†…çš„è‹±æ–‡å­—æ¯ï¼Œä¸¥æ ¼è¿‡æ»¤ä¹±ç """
    if not text:
        return ""
    
    # å»é™¤æ§åˆ¶å­—ç¬¦
    cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # æŒ‰å¥å·åˆ†å‰²ï¼Œé€å¥å¤„ç†
    sentences = []
    
    # å…ˆæŒ‰å¥å·åˆ†å‰²
    parts = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', cleaned)
    
    for part in parts:
        part = part.strip()
        if not part:
            continue
            
        # åªå¤„ç†åŒ…å«ä¸­æ–‡çš„éƒ¨åˆ†
        if re.search(r'[\u4e00-\u9fff]', part):
            # åŸºç¡€æ¸…ç†
            cleaned_part = re.sub(r'[^\u4e00-\u9fff\u0020-\u007e\u3000-\u303f\uff00-\uffef\s]+', ' ', part)
            cleaned_part = re.sub(r'ph333\s*', '', cleaned_part)
            cleaned_part = re.sub(r'\b[A-Za-z]{1}\b', '', cleaned_part)
            cleaned_part = re.sub(r'\s+', ' ', cleaned_part).strip()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºä¹±ç 
            if not _is_garbled_text(cleaned_part) and len(cleaned_part) > 8:
                sentences.append(cleaned_part)
    
    # åˆå¹¶å¥å­
    result = '. '.join(sentences)
    if result and not result.endswith('.'):
        result += '.'
    
    # æœ€ç»ˆæ¸…ç†
    result = re.sub(r'\s+', ' ', result)
    result = re.sub(r'[.,!?;:]{2,}', '.', result)
    
    return result.strip()

def test_wps_extraction():
    """æµ‹è¯•WPSæ–‡ä»¶æå–åŠŸèƒ½"""
    wps_file = "ç¥ç»ç½‘ç»œä»é›¶å®ç°æ•™ç¨‹.wps"
    
    if not os.path.exists(wps_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {wps_file}")
        return
    
    print("æ­£åœ¨æµ‹è¯•WPSæ–‡ä»¶æ–‡æœ¬æå–...")
    print("=" * 60)
    
    # ä»æœ¬åœ°è¯»å–æ–‡ä»¶ä¸ºå­—èŠ‚æµ
    with open(wps_file, 'rb') as file:
        byte_data = file.read()
        text = extract_text_from_wps_stream(byte_data)
    
    print(f"âœ… æå–æˆåŠŸ! æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print("æå–ç»“æœ:")
    print("-" * 60)
    print(text)
    print("-" * 60)
    
    # ä¿å­˜ç»“æœ
    with open("wps_clean_extracted_text.txt", "w", encoding="utf-8") as f:
        f.write(text)
    print(f"\nğŸ“„ å®Œæ•´æ–‡æœ¬å·²ä¿å­˜åˆ°: wps_clean_extracted_text.txt")

if __name__ == "__main__":
    test_wps_extraction()